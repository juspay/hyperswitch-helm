# Hyperswitch Monitoring Stack Configuration

# Override names for monitoring chart resources
# -- Override the base name for all resources (defaults to 'hyperswitch-monitoring')
nameOverride: null
# -- Override the full name for all resources (defaults to {release-name}-hyperswitch-monitoring)
fullnameOverride: null

# Global configuration
global:
  # Global image registry that can override all imageRegistry fields
  imageRegistry: null
  # Some charts use this format for global image configuration
  image:
    registry: null
  # Tolerations to be applied to all monitoring components
  tolerations: []
  labels:
    stack: "hyperswitch-monitoring"

# kube-prometheus-stack configuration
kube-prometheus-stack:
  enabled: true

  # Note: The kube-prometheus-stack subchart creates Prometheus/Alertmanager CRDs
  # The Prometheus Operator automatically prefixes StatefulSet names with the resource type:
  # - CRD name: {release-name}-kube-promethe-prometheus
  # - StatefulSet name: prometheus-{release-name}-kube-promethe-prometheus
  # This is expected behavior and ensures the operator can manage multiple instances

  # Override to use shorter, cleaner names
  # Format: {release-name}-prometheus (operator adds prefix: prometheus-{release-name}-prometheus)
  nameOverride: ""
  fullnameOverride: ""

  # Prometheus Operator configurations
  prometheusOperator:
    admissionWebhooks:
      enabled: true
      patch:
        podAnnotations:
          sidecar.istio.io/inject: "false"

  alertmanager:
    enabled: true
    # Use fullnameOverride to control the Alertmanager CRD name
    # The operator will create StatefulSet: alertmanager-{this-value}
    # Leave empty to use default: {release-name}-kube-promethe-alertmanager
    fullnameOverride: ""
    alertmanagerSpec:
      # Tolerations for Alertmanager pods
      tolerations: []

  # Prometheus configuration
  prometheus:
    enabled: true
    # Use fullnameOverride to control the Prometheus CRD name
    # The operator will create StatefulSet: prometheus-{this-value}
    # Leave empty to use default: {release-name}-kube-promethe-prometheus
    fullnameOverride: ""
    prometheusSpec:
      # Tolerations for Prometheus pods
      tolerations: []

      # Resource requests and limits
      resources:
        requests:
          memory: 256Mi
          cpu: 50m
        limits:
          memory: 1Gi
          cpu: 500m

      # Retention period
      retention: 30d

      # Empty selector - discovers all ServiceMonitors in allowed namespaces
      serviceMonitorSelector: {}

      # namespaces to monitor
      serviceMonitorNamespaceSelector: {}

      # Storage configuration
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 50Gi

      # Additional scrape configs from hyperswitch-stack
      additionalScrapeConfigs:
        - job_name: "kubernetes-pods"
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: "$1:$2"

        - job_name: "kubernetes-nodes"
          kubernetes_sd_configs:
            - role: node
          relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)

  # Grafana configuration
  grafana:
    enabled: true
    adminPassword: "admin"

    # Tolerations for Grafana pods
    tolerations: []

    # Grafana image
    image:
      tag: 10.0.1

    # Install additional plugins
    plugins:
      - volkovlabs-variable-panel

    # Disable default datasources from kube-prometheus-stack
    defaultDatasourceEnabled: false

    # Configure datasources - these will be processed as plain YAML, not templates
    additionalDataSources: []

    # Disable the default prometheus datasource
    prometheus:
      datasource:
        enabled: false

    # Sidecar configuration for datasources
    sidecar:
      datasources:
        enabled: true
        label: grafana_datasource
        labelValue: "1"
        # Search in the current namespace
        searchNamespace: "{{ .Release.Namespace }}"
        # Disable default behavior
        defaultDatasourceEnabled: false
        # Skip TLS verification for datasources
        skipTlsVerify: true
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: "1"
        # Search in all namespaces to find dashboards
        searchNamespace: ALL
        # Provider configuration
        provider:
          allowUiUpdates: false


# Loki configuration
loki:
  enabled: true
  # Note: fullnameOverride should include release name to avoid conflicts with ClusterRole
  # Use: --set loki.fullnameOverride={release-name}-loki when installing multiple releases
  fullnameOverride: ""
  global:
    image:
      registry: null

  # Loki configuration
  loki:
    auth_enabled: false

    server:
      http_listen_port: 3100
      grpc_listen_port: 9095

    commonConfig:
      replication_factor: 1

    storage:
      type: filesystem

    schemaConfig:
      configs:
        - from: 2024-01-01
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: index_
            period: 24h

    limits_config:
      retention_period: 168h
      ingestion_rate_mb: 10
      ingestion_burst_size_mb: 20

  # Single binary deployment
  deploymentMode: SingleBinary

  singleBinary:
    replicas: 1
    # Tolerations for Loki pods
    tolerations: []
    persistence:
      enabled: true
      size: 10Gi
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi

  # Explicitly disable other deployment modes
  read:
    replicas: 0
  write:
    replicas: 0
  backend:
    replicas: 0
  gateway:
    replicas: 0

  # Disable chunks cache to reduce memory requirements
  chunksCache:
    enabled: false
  resultsCache:
    enabled: false

  # Loki Canary configuration
  # Note: Loki canary does not support fullnameOverride and always uses "loki-canary" as the base name
  # This results in DaemonSet name: loki-canary (without release prefix)
  # This is a known limitation of the Loki chart
  lokiCanary:
    enabled: true
    kind: DaemonSet

# Promtail configuration
promtail:
  enabled: true

  # Tolerations for Promtail pods
  tolerations: []

  config:
    clients:
      - url: "http://loki:3100/loki/api/v1/push"

    snippets:
      pipelineStages:
        - cri: {}

      # Extra relabel configs to filter hyperswitch pods
      extraRelabelConfigs:
        - action: "keep"
          regex: "hyperswitch-.*"
          source_labels: ["__meta_kubernetes_pod_label_app"]

# OpenTelemetry Collector configuration
opentelemetry-collector:
  enabled: true

  mode: "deployment"

  # Specify which namespace should be used to deploy the resources into
  namespaceOverride: ""

  presets:
    kubernetesAttributes:
      enabled: true
      extractAllPodLabels: true
      extractAllPodAnnotations: false

  alternateConfig:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317

    processors:
      batch: {}
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25
      # Adds resource attributes (added by k8sattributes processor) as metrics labels
      transform:
        metric_statements:
          - context: datapoint
            statements:
              - set(attributes["source_namespace"], resource.attributes["k8s.namespace.name"])
              - set(attributes["source_pod"], resource.attributes["k8s.pod.name"])
              - set(attributes["source_app"], resource.attributes["app"])
              - set(attributes["source_version"], resource.attributes["version"])

    exporters:
      debug:
        verbosity: detailed
      prometheus:
        endpoint: ${env:MY_POD_IP}:9898

    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133

    service:
      telemetry:
        logs:
          level: DEBUG
          encoding: json
        metrics:
          level: detailed
          address: ${env:MY_POD_IP}:8888
      extensions:
        - health_check
      pipelines:
        metrics:
          receivers:
            - otlp
          processors:
            - memory_limiter
            - transform
            - batch
          exporters:
            - prometheus

  image:
    repository: otel/opentelemetry-collector-contrib
    tag: 0.122.1

  nodeSelector: {}
  tolerations: []
  affinity: {}

  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otel-metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
    hs-metrics:
      enabled: true
      containerPort: 9898
      servicePort: 9898
      protocol: TCP
    otlp-http:
      enabled: false
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false

  resources:
    # Due to the `k8sattributes` processor, the collector uses more than 2 GiB of RAM and 1000+m vCPU  at startup,
    # and later drops to ~300 MiB of RAM and ~200m vCPU.
    limits:
      cpu: 1500m
      memory: 4Gi
    requests:
      cpu: 250m
      memory: 512Mi

  replicaCount: 1

  serviceMonitor:
    enabled: true
    metricsEndpoints:
      - port: otel-metrics
        honorLabels: true
        interval: 30s
        path: /metrics
      - port: hs-metrics
        honorLabels: true
        interval: 15s
        path: /metrics

# https://artifacthub.io/packages/helm/bitnami/postgresql
postgresql:
  # @section -- Dependencies configuration for grafana datasource
  # When external is false, the host will be prefixed with {{ .Release.Name }}- (e.g., hypers-v1-postgresql)
  external: false
  primary:
    host: "postgresql"
    port: 5432
    username: "hyperswitch"
    password: "ZGJwYXNzd29yZDEx"
    database: "hyperswitch"

# Grafana ingress configuration
grafana:
  ingress:
    enabled: true
    ingressClassName: alb
    annotations:
      alb.ingress.kubernetes.io/backend-protocol: HTTP
      alb.ingress.kubernetes.io/backend-protocol-version: HTTP1
      alb.ingress.kubernetes.io/group.name: hyperswitch-monitoring-alb-ingress-group
      alb.ingress.kubernetes.io/ip-address-type: ipv4
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
      alb.ingress.kubernetes.io/load-balancer-name: hyperswitch-monitoring
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/security-groups: "loadbalancer-sg"
      alb.ingress.kubernetes.io/tags: stack=hyperswitch-monitoring
      alb.ingress.kubernetes.io/target-type: ip
    hosts:
      - host: ""
        paths:
          - path: /
            pathType: Prefix
    tls: []

# Load balancer configuration
loadBalancer:
  targetSecurityGroup: "loadbalancer-sg"
